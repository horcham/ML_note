\section{决策树}
决策树学习，假设给定训练数据集
\begin{eqnarray}
D=\{ (\sample{x}{1},\sample{y}{1}),(\sample{x}{2},\sample{y}{2}),\cdots,(\sample{x}{N},\sample{y}{N}) \}
\end{eqnarray}
其中，$\sample{x}{i}=(\samplet{x}{i}{1},\samplet{x}{i}{2},\cdots,\samplet{x}{i}{n})$，$n$为特征个数，$\sample{y}{i}\in\{ 1,2,\cdots,K \}$，$K$为类别数目，$i=1,2,\cdots,N$，$N$为样本容量。
\subsection{特征选择}
\subsubsection{信息增益}
设$X$是一个取有限个值的离散随机变量，其概率分布为
\begin{eqnarray}
P(X=\sample{x}{i})=p_i,i=1,2,\cdots,n
\end{eqnarray}
则随机变量$X$的熵定义为
\begin{eqnarray}
H(X)=-\sum_{i=1}^n p_i\log p_i
\end{eqnarray}
熵越大，随机变量的不确定性就越大。

设有随机变量$(X,Y)$，其联合概率分布为
\begin{eqnarray}
P(X=\sample{x}{i},Y=\sample{y}{i})=p_{ij}，i,2,\cdots,n;j=1,2,\cdots,m
\end{eqnarray}
条件熵$H(Y|X)$为已知随机变量$X$的条件下随机变量$Y$的不确定性，随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$定义如下
\begin{eqnarray}
H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)
\end{eqnarray}
其中，$p_i=P(X=x_i),i=1,2,\cdots,n$。

信息增益（information gain）表示得知特征$X$的信息，是的类$Y$的信息的不确定性减少的程度，定义如下
\paragraph{信息增益}特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即
\begin{eqnarray}
g(D,A)=H(D)-H(D|A)
\end{eqnarray}

根据信息增益准则的特征选择方法是：对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

设训练数据为$D$，$|D|$表示其样本容量，即样本个数，设有$K$个类$C_k$，$k=1,2,\cdots,K$，$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K|C_k|=|D|$。设特征$A$有$n$个不同的取值$\{ a_1,a_2,\cdots,a_n \}$，记特征集为$A$，根据某一特征$a$的取值将$D$划分为$n$个子集$D_1,D_2,\cdots,D_n$，$|D_i|$为$D_i$的样本个数，$\sum_{i=1}^n|D_i|=|D|$。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，即$D_{ik}=D_i\cap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数，信息增益算法如下
\begin{itemize}
\item 输入：训练数据集$D$和特征$a$；
\item 输出：特征$a$对训练数据集$D$的信息增益$g(D,a)$
\item[1] 计算数据集$D$的经验熵$H(D)$
\begin{eqnarray}
H(D)=-\sum_{k=1}^K \frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}
\end{eqnarray}
\item[2] 计算特征$a$对数据集$D$的经验条件熵$H(D|a)$
\begin{eqnarray}
\begin{aligned}
H(D|a)&=\sum_{i=1}^n \frac{|D_i|}{|D|}H(D|a=a_i)\\
&=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)\\
&=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}
\end{aligned}
\end{eqnarray}
\item[3] 计算信息增益
\begin{eqnarray}
g(D,A)=H(D)-H(D|a)
\end{eqnarray}
\end{itemize}

于是，在候选属性集合$A$中，选择使得划分后信息增益最大的属性作为最优划分属性，即
\begin{eqnarray}
a_*=\arg\max_{a\in A}g(D,a)
\end{eqnarray}

该算法天生偏向选择分支多的属性，容易导致过拟合。

\subsubsection{信息增益比}
\paragraph{信息增益比}特征$a$对训练数据集$D$的信息增益比$g_R(D,a)$定义为其信息增益$g(D,a)$与训练数据集$D$关于特征$a$的值的熵$H_a(D)$之比，即
\begin{eqnarray}
g_R(D,a)=\frac{g(D,a)}{H_a(D)}
\end{eqnarray}
其中，$H_a(D)=-\sum_{i=1}^n \frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$，$n$是特征$a$取值的个数。

于是，在候选属性集合$A$中，选择使得划分后信息增益最大比的属性作为最优划分属性，即
\begin{eqnarray}
a_*=\arg\max_{a\in A}g_R(D,a)
\end{eqnarray}

\subsubsection{基尼指数}
分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为
\begin{eqnarray}
Gini(p)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^K p_k^2
\end{eqnarray}
对于样本集合$D$，$D$的纯度可以用Gini指数来度量
\begin{eqnarray}
Gini(D)=1-\sum_{k=1}^K \left( \frac{|C_k|}{|D|} \right)^2
\end{eqnarray}
其中，$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。直观上，$Gini(D)$反映了$D$中随机抽取两个样本，其类别标记不一致的概率。因此，$Gini(D)$越小，则数据集$D$的纯度越高。

设特征$a$有$n$个不同的取值$\{ a_1,a_2,\cdots,a_n \}$，根据特征$a$的取值将$D$划分为$n$个子集$D_1,D_2,\cdots,D_n$，$|D_i|$为$D_i$的样本个数，$\sum_{i=1}^n|D_i|=|D|$，则属性$a$的Gini指数定义为
\begin{eqnarray}
Gini(D,a)=\sum_{i=1}^n \frac{|D_i|}{|D|}Gini(D_i)
\end{eqnarray}
性$a$的Gini指数$Gini(D,a)$表示经$a$分后集合$D$的不确定性，则$Gini$指数值越大，样本集合的不确定性就越大。

于是，在候选属性集合$A$中，选择使得划分后$Gini$指数最小的属性作为最优划分属性，即
\begin{eqnarray}
a_*=\arg\min_{a\in A}Gini(D,a)
\end{eqnarray}

\subsection{决策树的生成}
\subsubsection{ID3算法}
从根节点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同区直建立子节点，再递归地使用以上方法，构造决策树，直到所有特征的信息增益均最小或没有特征可以选择为止，最后得到一个决策树
\paragraph{ID3算法}
\begin{itemize}
\item 输入：训练数据集$D$，特征集$A$，阈值$\epsilon$
\item 输出：决策树$T$
\item[1] 若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$
\item[2] 若$A=\emptyset$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$
\item[3] 否则，计算$A$中各特征对$D$的信息增益$g(D,A_i)$，选择信息增益最大的特征$A_g$
\item[4] 如果$A_g$的信息增益小于阈值$\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$
\item[5] 否则，对$A_g$的每一个可能取值$a_i$，依$a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由节点及其子结点构成树$T$，返回$T$
\item[6] 对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用$1\sim 5$，得到子树$T_i$，返回$T_i$
\end{itemize}





