\section{决策树}
决策树学习，假设给定训练数据集
\begin{eqnarray}
D=\{ (\sample{x}{1},\sample{y}{1}),(\sample{x}{2},\sample{y}{2}),\cdots,(\sample{x}{N},\sample{y}{N}) \}
\end{eqnarray}
其中，$\sample{x}{i}=(\samplet{x}{i}{1},\samplet{x}{i}{2},\cdots,\samplet{x}{i}{n})$，$n$为特征个数，$\sample{y}{i}\in\{ 1,2,\cdots,K \}$，$K$为类别数目，$i=1,2,\cdots,N$，$N$为样本容量。
\subsection{特征选择}
\subsubsection{信息增益}
设$X$是一个取有限个值的离散随机变量，其概率分布为
\begin{eqnarray}
P(X=\sample{x}{i})=p_i,i=1,2,\cdots,n
\end{eqnarray}
则随机变量$X$的熵定义为
\begin{eqnarray}
H(X)=-\sum_{i=1}^n p_i\log p_i
\end{eqnarray}
熵越大，随机变量的不确定性就越大。

设有随机变量$(X,Y)$，其联合概率分布为
\begin{eqnarray}
P(X=\sample{x}{i},Y=\sample{y}{i})=p_{ij}，i,2,\cdots,n;j=1,2,\cdots,m
\end{eqnarray}
条件熵$H(Y|X)$为已知随机变量$X$的条件下随机变量$Y$的不确定性，随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$定义如下
\begin{eqnarray}
H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)
\end{eqnarray}
其中，$p_i=P(X=x_i),i=1,2,\cdots,n$。

信息增益（information gain）表示得知特征$X$的信息，是的类$Y$的信息的不确定性减少的程度，定义如下
\paragraph{信息增益}特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即
\begin{eqnarray}
g(D,A)=H(D)-H(D|A)
\end{eqnarray}

根据信息增益准则的特征选择方法是：对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

设训练数据为$D$，$|D|$表示其样本容量，即样本个数，设有$K$个类$C_k$，$k=1,2,\cdots,K$，$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K|C_k|=|D|$。设特征$A$有$n$个不同的取值$\{ a_1,a_2,\cdots,a_n \}$，记特征集为$A$，根据某一特征$a$的取值将$D$划分为$n$个子集$D_1,D_2,\cdots,D_n$，$|D_i|$为$D_i$的样本个数，$\sum_{i=1}^n|D_i|=|D|$。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，即$D_{ik}=D_i\cap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数，信息增益算法如下
\begin{itemize}
\item 输入：训练数据集$D$和特征$a$；
\item 输出：特征$a$对训练数据集$D$的信息增益$g(D,a)$
\item[1] 计算数据集$D$的经验熵$H(D)$
\begin{eqnarray}
H(D)=-\sum_{k=1}^K \frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}
\end{eqnarray}
\item[2] 计算特征$a$对数据集$D$的经验条件熵$H(D|a)$
\begin{eqnarray}
\begin{aligned}
H(D|a)&=\sum_{i=1}^n \frac{|D_i|}{|D|}H(D|a=a_i)\\
&=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)\\
&=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}
\end{aligned}
\end{eqnarray}
\item[3] 计算信息增益
\begin{eqnarray}
g(D,A)=H(D)-H(D|a)
\end{eqnarray}
\end{itemize}

于是，在候选属性集合$A$中，选择使得划分后信息增益最大的属性作为最优划分属性，即
\begin{eqnarray}
a_*=\arg\max_{a\in A}g(D,a)
\end{eqnarray}

该算法天生偏向选择分支多的属性，容易导致过拟合。

\subsubsection{信息增益比}
\paragraph{信息增益比}特征$a$对训练数据集$D$的信息增益比$g_R(D,a)$定义为其信息增益$g(D,a)$与训练数据集$D$关于特征$a$的值的熵$H_a(D)$之比，即
\begin{eqnarray}
g_R(D,a)=\frac{g(D,a)}{H_a(D)}
\end{eqnarray}
其中，$H_a(D)=-\sum_{i=1}^n \frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$，$n$是特征$a$取值的个数。

于是，在候选属性集合$A$中，选择使得划分后信息增益最大比的属性作为最优划分属性，即
\begin{eqnarray}
a_*=\arg\max_{a\in A}g_R(D,a)
\end{eqnarray}

\subsubsection{Gini指数}
分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的Gini指数定义为
\begin{eqnarray}
Gini(p)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^K p_k^2
\end{eqnarray}
对于样本集合$D$，$D$的纯度可以用Gini指数来度量
\begin{eqnarray}
Gini(D)=1-\sum_{k=1}^K \left( \frac{|C_k|}{|D|} \right)^2
\end{eqnarray}
其中，$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。直观上，$Gini(D)$反映了$D$中随机抽取两个样本，其类别标记不一致的概率。因此，$Gini(D)$越小，则数据集$D$的纯度越高。

设特征$a$有$n$个不同的取值$\{ a_1,a_2,\cdots,a_n \}$，根据特征$a$的取值将$D$划分为$n$个子集$D_1,D_2,\cdots,D_n$，$|D_i|$为$D_i$的样本个数，$\sum_{i=1}^n|D_i|=|D|$，则属性$a$的Gini指数定义为
\begin{eqnarray}
Gini(D,a)=\sum_{i=1}^n \frac{|D_i|}{|D|}Gini(D_i)
\end{eqnarray}
性$a$的Gini指数$Gini(D,a)$表示经$a$分后集合$D$的不确定性，则$Gini$指数值越大，样本集合的不确定性就越大。

于是，在候选属性集合$A$中，选择使得划分后$Gini$指数最小的属性作为最优划分属性，即
\begin{eqnarray}
a_*=\arg\min_{a\in A}Gini(D,a)
\end{eqnarray}

Gini指数是熵的简化版。

\subsection{决策树的生成}
\subsubsection{ID3}
从根节点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同区直建立子节点，再递归地使用以上方法，构造决策树，直到所有特征的信息增益均最小或没有特征可以选择为止，最后得到一个决策树
\paragraph{ID3算法}
\begin{itemize}
\item 输入：训练数据集$D$，特征集$A$，阈值$\epsilon$
\item 输出：决策树$T$
\item[1] 若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$
\item[2] 若$A=\emptyset$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$
\item[3] 否则，计算$A$中各特征对$D$的信息增益$g(D,A_i)$，选择信息增益最大的特征$A_g$
\item[4] 如果$A_g$的信息增益小于阈值$\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$
\item[5] 否则，对$A_g$的每一个可能取值$a_i$，依$a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由节点及其子结点构成树$T$，返回$T$
\item[6] 对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用$1\sim 5$，得到子树$T_i$，返回$T_i$
\end{itemize}

\subsubsection{C4.5}
C4.5采用信息增益比来选择特征
\paragraph{C4.5算法}
\begin{itemize}
\item 输入：训练数据集$D$，特征集$A$，阈值$\epsilon$
\item 输出：决策树$T$
\item[1] 若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$
\item[2] 若$A=\emptyset$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$
\item[3] 否则，计算$A$中各特征对$D$的信息增益比$g(D,A_i)$，选择信息增益比最大的特征$A_g$
\item[4] 如果$A_g$的信息增益比小于阈值$\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$
\item[5] 否则，对$A_g$的每一个可能取值$a_i$，依$a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由节点及其子结点构成树$T$，返回$T$
\item[6] 对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用$1\sim 5$，得到子树$T_i$，返回$T_i$
\end{itemize}

\subsubsection{CART}
对回归树用平方误差最小化准则，对分类树用Gini指数最小化准则，进行特征选择，生成二叉树。

\paragraph{回归树的生成}

假设$X$和$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集$ D=\{ (\sample{x}{1},\sample{y}{1}),(\sample{x}{2},\sample{y}{2}),$ $\cdots,(\sample{x}{N},\sample{y}{N}) \} $。回归树将输入空间划分为$M$个单元$D_1,D_2,\cdots,D_M$，且在每个单元上有一个固定值$c_m$，因此回归树模型表示为
\begin{eqnarray}
f(x)=\sum_{m=1}^M c_m1(x\in D_m)
\end{eqnarray}
其中，$1(x)$为示性函数。具体的，为求解
\begin{eqnarray}
\min_{j,s}
\left(
	\min_{c_1}\sum_{x_i\in D_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in D_2(j,s)}(y_i-c_2)^2
\right)
\end{eqnarray}
其中，$j$为最优划分变量，$s$为最优划分点。$D_1(j,s)=\{x|x_j\leq s\}$，$D_2(j,s)=\{x|x_j> s\}$。对于$j$和$s$的选取，采用遍历的方法。遍历划分变量$j$，再以步长$\Delta s$扫描划分点$s$。对于$j$，$s$都固定，且用平方误差$\sum_{x_i\in D_m}(\sample{y}{i}-f(\sample{x}{i}))^2$来表示回归树对于训练数据的预测误差，则可求得在单元$D_m$上的$c_m$的最优值$\hat{c_m}$为
\begin{eqnarray}
\hat{c_m}=\frac{1}{N_m}\sum_{x_i\in D_m(j,s)}y_i,\ m=1,2
\end{eqnarray}
其中，$N_m$为单元$D_m$中的样本个数。其表示对$D_m$的所有样本的$y$取均值。

经过两轮遍历之后，即可选出最优划分变量和最优划分点，以及计算出对应的$\hat{c_m}$。算法如下

\subparagraph{最小二乘回归树生成算法}
\begin{itemize}
\item 输入：训练数据集$D$，特征集$A$，阈值$\epsilon$
\item 输出：回归树$f(x)$
\item[1] 若$D$中所有实例的输出均为$y_0$，则$T$为单节点树，并将$y_0$作为该结点的输出值，返回$T$
\item[2] 否则，采用遍历的方法。遍历划分变量$j$，对于固定的划分变量$j$再以步长$\Delta s$扫描划分点$s$，对于$j$，$s$都固定，求$c_1$，$c_2$
\begin{eqnarray}
c_m=\frac{1}{N_m}\sum_{x_i\in D_m(j,s)}y_i,\ m=1,2
\end{eqnarray}
来求解
\begin{eqnarray}
\min_{j,s}
\left(
	\min_{c_1}\sum_{x_i\in D_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in D_2(j,s)}(y_i-c_2)^2
\right)
\end{eqnarray}
并记损失函数
\begin{eqnarray}
L=\sum_{x_i\in D_1(j,s)}(y_i-c_1)^2+\sum_{x_i\in D_2(j,s)}(y_i-c_2)^2
\end{eqnarray}
\item[3] 如果$L$小于阈值$\epsilon$，则置$T$为单结点树，并将$D$中实例的输出的均值$\hat{c}=\frac{1}{N_{|D|}}\sum_{x_i\in D}y_i$作为该结点的输出值，返回$T$
\item[4] 否则，用选定的对$(j,s)$划分区域，并决定相应的输出值
\begin{eqnarray}
D_1(j,s)&=&\{x|x_j\leq s\}\\
D_2(j,s)&=&\{x|x_j> s\}\\
\hat{c_m}&=&\frac{1}{N_m}\sum_{x_i\in D_m(j,s)}y_i,\ m=1,2
\end{eqnarray}
\item[5] 对这两个子区域$D_i,\ i=1,2$，以$D_i$为训练集，以$A$为特征集，递归地调用$1\sim 5$，得到子树$T_i$，返回$T_i$
\item[6] 将输入空间划分为$M$个区域$D_1,D_2,\cdots,D_M$，生成决策树
\begin{eqnarray}
f(x)=\sum_{m=1}^M c_m1(x\in D_m)
\end{eqnarray}
\end{itemize}


注：对比ID3和C4.5算法，由于回归树少了将特征集进行剔除，即少了第5步以$A-\{A_g\}$为特征集，因而少了这一步：
\begin{itemize}
\item[2] 若$A=\emptyset$，则$T$为单节点树，并将$D$中实例的输出的均值$\hat{c}=\frac{1}{N_{|D|}}\sum_{x_i\in D}y_i$作为该结点的输出值，返回$T$
\end{itemize}


\paragraph{分类树的生成}

分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点，
\subparagraph{CART生成算法}
\begin{itemize}
\item 输入：训练数据集$D$，特征集$A$，Gini指数阈值$\epsilon_g$，样本个数阈值$\epsilon_n$
\item 输出：CART决策树
\item[1] 若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$
\item[2] 若$D$中所有实例个数小于样本个数阈值$\epsilon_n$，则取$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$
\item[3] 若$A=\emptyset$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$
\item[4] 否则，计算$A$中各特征对$D$的Gini指数。计算时，对每一个特征$A_i$，该特征每个可能取的值$a_j$，根据$A$是否等于$a_j$，将样本点$D$分割成$D_1$，$D_2$两部分，即
\begin{eqnarray}
D_1=\{ (x,y)\in D|A_i=a_j \},\ \ D_2 = D-D_1
\end{eqnarray}
分割之后，计算在特征$A_i=a_j$时，集合$D$的Gini指数
\begin{eqnarray}
Gini(D,A_i=a_j)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
\end{eqnarray}
计算所有特征$A$以及所有可能取值$a$的Gini指数。选择Gini指数最小的特征$A^*$及其对应的切分点$a^*$分别作为最优特征与最优切分点，其对应的Gini指数为$Gini_{A^*}$。
\item[5] 如果$A^*$的Gini指数$Gini_{A^*}$小于阈值$\epsilon_g$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$
\item[6] 否则，对最优特征$A^*$的最优划分$a^*$，依是否为$a^*$，将$D$分割为两个非空子集$D_1$，$D_2$，将$D_i,\ i=1,2$中实例数最大的类作为标记，构建子结点，由节点及其子结点构成树$T$，返回$T$
\item[7] 对第$i$个子结点，以$D_i,\ i=1,2$为训练集，递归地调用$1\sim 6$，得到子树$T_i$，返回$T_i$
\end{itemize}

\subsection{决策树的剪枝}
\subsubsection{预剪枝}
在特征选择后，选定了将采用特征$a$进行下一步划分生成子树。在这之前，先通过预剪枝，比较划分前和划分后对与验证集的准确率。若划分前的验证集的准确率大于等于划分后的验证集的准确率，则进行预剪枝，即不做下一步划分，否则做下一步划分。
\begin{itemize}
\item 输入：验证数据集$D_t$，最优特征$a$
\item 是否按照最优特征$a$进行下一步划分
\item[1] 取$D_t$中实例数最大的类$C_k$作为该节点的类标记，验证集准确率为
\begin{eqnarray}
acc_{D_t}=\frac{1}{|D_t|}\sum_{x_i\in D_t}1\{y_i=C_k\}
\end{eqnarray}
\item[2] 按照最优特征$a$，将$D_t$分割成若干个非空子集$D_{t1},D_{t2},\cdots,D{tn}$，对于每个子集$D_{ti}$，取$D_{ti}$中实例数最大的类$C_{ti}$作为该节点的类标记，计算验证集精度
\begin{eqnarray}
acc_{D_{t\cdot}}=\frac{1}{|D_t|}\sum_{i=1}^n\sum_{x_j\in D_{ti}}1\{ y_j=C_{ti} \}
\end{eqnarray}
\item[3] 比较$acc_{D_t}$和$acc_{D_{t\cdot}}$。若$acc_{D_t}<acc_{D_{t\cdot}}$，则按照最优特征$a$进行下一步划分；$acc_{D_t}\geq acc_{D_{t\cdot}}$，则不进行下一步划分
\end{itemize}
\subsubsection{后剪枝}
\paragraph{代价复杂剪枝}设树$T$的叶节点个数为$|T|$，$t$是树$T$的叶节点，该节点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,\cdots,K$，$H_t(T)$为叶节点$t$上的经验熵，$\alpha\geq 0$为参数，定义决策树学习的损失函数为
\begin{eqnarray}
C_\alpha(T)=\sum_{t=1}^{|T|} N_tH_t(T)+\alpha|T|
\end{eqnarray}
其中，经验熵为
\begin{eqnarray}
H_t(T)=-\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}
\end{eqnarray}
将损失函数右端第一项记为、
\begin{eqnarray}
C(T)=\sum_{t=1}^{|T|} N_th_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^K N_{tk}\log\frac{N_{tk}}{N_t}
\end{eqnarray}
于是有
\begin{eqnarray}
C_\alpha(T)=C(T)+\alpha|T|
\end{eqnarray}
其中，$C(T)$为模型对训练数据的预测误差，$|T|$为模型复杂度，参数$\alpha\geq	 0$控制两者之间的影响，大的$\alpha$偏向于使用简单的模型，小的$\alpha$偏向于使用复杂的模型。
\subparagraph{代价复杂剪枝}
\begin{itemize}
\item 输入：生成算法产生的整个树$T$，参数$\alpha$
\item 输出：修剪后的子树$T_\alpha$
\item[1] 计算每个节点的经验熵
\item[2] 递归地从树的叶节点向上回缩。设一组叶节点回缩到其父节点之前与之后的整体树分别为$T_B$与$T_A$，对应的损失函数分别为$C_\alpha(T_B)$与$C_\alpha{T_A}$，若
\begin{eqnarray}
C_\alpha(T_A)\leq C_\alpha(T_B)
\end{eqnarray}
则进行剪枝，即将父节点变为新的叶节点。
\item[3] 返回(2)，知道不能继续为止，得到损失函数最小的子树$T_\alpha$
\end{itemize}
\paragraph{误差降低剪枝}
\begin{itemize}
\item 输入：生成算法产生的整个树$T$
\item 输出：修剪后的子树$T_\alpha$
\item[1] 计算未回缩之前的树（记为$T_B$）的误差$C(T_B)$
\item[2] 递归地从树的叶节点向上回缩，计算回缩之后的树（记为$T_A$）的误差$C(T_A)$。如果
\begin{eqnarray}
C(T_A)\leq C(T_B)
\end{eqnarray}
则进行剪枝。
\item[3] 重复进行步骤$1\sim 2$，知道不能继续为止，得到损失函数最小的子树$T$
\end{itemize}
