\section{贝叶斯分类器}
\subsection{贝叶斯决策论}
假设有$N$种可能的类别标记，即$\mathcal{Y}=\{c_1,c_2,\cdots,c_N\}$，$\lambda_{ij}$为$c_j$的样本误分类为$c_i$所产生的损失。基于后验概率$P(c_i|x)$可获得将样本$x$分类为$c_i$产生的期望损失（expected loss），即在样本$x$上的“条件风险”（conditional risk）
\begin{eqnarray}
R(c_i|x)=\sum_{j=1}^N \lambda_{ij}P(c_j|x)
\end{eqnarray}
条件风险可以理解为，对于给定的$x$，其真实是$c_j$类的概率为$P(c_j|x)$。则对于这个$x$来，猜测是属于$c_i$类，则$\lambda_{ij}$为$c_j$的样本误分类为$c_i$所产生的损失。

任务是寻找一个判定准则$h$，来最小化总体风险
\begin{eqnarray}
R(h)=E[R(h(x)|x)]
\end{eqnarray}
其中，$E[]$为数学期望。贝叶斯判定准则（Bayes decision rule）为：为最小化总体风险，只需在每个样本上选择能使条件风险$R(c|x)$最小的类别标记，即
\begin{eqnarray}
h^\star(x)=\arg\min_{c\in\mathcal{Y}}R(c|x)
\end{eqnarray}
此时$h^\star$成为贝叶斯最优分类器（Bayes optimal classifer），对应的总体风险为$R(h^\star)$称为贝叶斯风险（Bayes risk）。$1-R(h^\star)$反映了分类器所能达到的最好性能。 

具体的，令
\begin{eqnarray}
\lambda_{ij}=
\left\lbrace
\begin{aligned}
0 &,\ \ if\ i=j\\
1 &,\ \ otherwise
\end{aligned}
\right.
\end{eqnarray}
则对于$x$，条件风险为
\begin{eqnarray}
R(c_i|x) &=& \sum_{j=1}^N\lambda_{ij}P(c_j|x)\\
&=& \sum_{j=1}^N 1(c_i\neq c_j)P(c_j|x)\\
&=& 1-P(c_i|x)
\end{eqnarray}
其中，$c_i\in \mathcal{Y}$，可简写为
\begin{eqnarray}
R(c|x)=1-P(c|x)
\end{eqnarray}
因而贝叶斯最优分类器的求解为
\begin{eqnarray}
h^\star(x)=\arg\max_{c\in\mathcal{Y}}P(c|x)
\end{eqnarray}
需要估计$P(c|x)$，有以下两种策略
\begin{itemize}
\item 对于给定$x$，通过直接建模$P(c|x)$来预测$c$，这样得到的是判别式模型（discrimininative models）
\item 对联合概率分布$P(x,c)$建模，然后再由此得到$P(c|x)$，这样得到的是生成式模型（generative models）
\end{itemize}
对于生成式模型，考虑
\begin{eqnarray}
P(c|x) &=& \frac{P(x,c)}{P(c)}\\
&=& \frac{P(c)P(x|c)}{P(x)}
\end{eqnarray}
我们称$P(c)$为类的先验（prior）概率；$P(x|c)$样本$x$相对于类$c$的类条件概率（class-conditional probability）或称为似然（likelihood）；$P(x)$为用于归一化的证据（evidence）因子。由于$P(x)$与类标记无关，因而问题转化为求$P(c)$和$P(x|c)$。

对于类先验概率$P(c)$的估计，可用各类样本出现的频率来进行估计。

对于类条件概率$P(x|c)$的估计，涉及到关于$x$的所有属性的联合概率分布，样本的属性组合数通常大于训练样本数，且很多情况是不会在训练集中出现，因而直接根据样本出现的频率来估计有严重的困难，因而有下列求法。

\subsection{极大似然估计}
估计$P(x|c)$，设$D_c$表示训练集$D$中第$c$类样本组合成的集合，设$D_c$中的样本服从某个分布（如高斯分布），被参数向量$\theta_c$唯一确定，记$P(x|\theta_c)$。则通过极大似然估计求解出$\theta_c$，则可得到$D_c$的样本所服从的分布，则解出$P(x|c)$。

估计$P(x|c)$，设$D_c$表示训练集$D$中第$c$类样本组合成的集合，且假设这些样本是独立同分布的，则参数$\theta_c$对于数据集$D_c$的似然是
\begin{eqnarray}
L(\theta_c)=P(D_c|\theta_c)=\prod_{x\in D_c}P(x|\theta_c)
\end{eqnarray}
取对数似然（log-likelihood），有
\begin{eqnarray}
\begin{aligned}
LL(\theta_c) &= \log P(D_c|\theta_c)\\
&=\sum_{x\in D_c}\log P(x|\theta_c)
\end{aligned}
\end{eqnarray}
参数$\theta_c$的极大似然估计$\hat{\theta_c}$为
\begin{eqnarray}
\hat{\theta_c}=\arg\max_{\theta_c}LL(\theta_c)
\end{eqnarray}

假设$P(x|c)$服从多元高斯分布，则$P(x|c)\sim \mathcal{N}(\mu_c,\Sigma_c)$，参数$\mu_c$和$\Sigma_c$的极大似然估计为
\begin{eqnarray}
\hat{\mu_c}&=&\frac{1}{|D_c|}\sum_{x\in D_c}x\\
\hat{\Sigma_c}&=&\frac{1}{|D_c|}\sum_{x\in D_c}(x-\hat{\mu_c})^T(x-\hat{\mu_c})
\end{eqnarray}
则有
\begin{eqnarray}
p(x|c)=\frac{1}{(2\pi)^{\frac{m}{2}}}\frac{1}{|\hat{\Sigma_c}|^\frac{1}{2}}\exp
\left\lbrace
-\frac{1}{2}(x-\hat{\mu_c})^T\hat{\Sigma_c}^{-1}(x-\hat{\mu_c})\right\rbrace
\end{eqnarray}
其中，$m$指$x$的维度。

这种参数化的方法可以使类条件概率估计变得相对简单，但估计结果的准确性依赖于所猜测的分布是不是对的，这旺旺需要一定程度上利用关于应用任务本身的经验知识。否则若仅凭猜测来假设概率分布，往往得到误导性的结果。

\subsection{朴素贝叶斯分类器}
朴素贝叶斯分类器采用了“属性条件独立性假设（attribute conditional independence assumption）”：对已知类别，假设所有属性相对独立，即假设每个属性独立地对分类结果产生影响。则有
\begin{eqnarray}
\begin{aligned}
P(c|x)&=\frac{P(c)P(x|c)}{P(x)}\\
&=\frac{P(c)}{P(x)}\prod_{i=1}^dP(x_i|c)
\end{aligned}
\end{eqnarray}
其中，$d$为属性数目，$x_i$为$x$在第$i$个属性上的取值。由于$P(x)$不依赖于$c$，则最优的贝叶斯分类器为
\begin{eqnarray}
h(x)=\arg\max_{c\in\mathfrak{Y}}P(c)\prod_{i=1}^dP(x_i|c)
\end{eqnarray}
训练过程即为通过训练集$D$来估计类先验概率$P(c)$，并对每个属性估计条件概率$P(x_i|c)$

对于$P(c)$的求解，令$D_c$表示训练集$D$中第$c$类样本组成的集合，若有充足的独立同分布样本，则$P(c)$可估计为
\begin{eqnarray}
P(c)=\frac{|D_c|}{|D|}
\end{eqnarray}

对于$P(x_i|c)$的求解，分为两种情况，分别为离散属性和连续属性
\begin{itemize}
\item 对于离散属性而言，令$D_{c,x_i}$表示$D_c$中第$i$个属性上取值为$x_i$的样本组成的集合，则条件概率$P(x_i|c)$可估计为
\begin{eqnarray}
P(x_i|c)=\frac{|D_{c,x_i}|}{|D_c|}
\end{eqnarray}
\item 对连续属性而言，可考虑概率密度函数，假定$P(x_i,c)\sim\mathcal{N}(\mu_{c,i},\sigma_{c,i}^2)$。其中，$\mu_{c,i},\sigma_{c,i}^2$分别为第$c$类样本在第$i$个属性上取值的均值和方差，则有
\begin{eqnarray}
p(x_i,c)=\frac{1}{\sqrt{2\pi}\sigma_{c,i}}\exp
\left\lbrace
\begin{aligned}
-\frac{(x_i-\mu_{c,i})^2}{2\sigma_{c,i}^2}
\end{aligned}
\right\rbrace
\end{eqnarray}
\end{itemize}
为了避免其他属性携带的信息因为在训练集中未出现的属性值“抹去”，在估计概率值时通常要进行“平滑（smoothing）”，常用“拉普拉斯修正（Laplacian correction）”。令$N$表示训练集$D$中可能的类别数，$N_i$表示第$i$个属性可能的取值数，则将上式修正为
\begin{eqnarray}
\hat{P(c)}&=&\frac{|D_c|+1}{|D|+N}\\
\hat{P(x_i|c)}&=&\frac{|D_{c,x_i}|+1}{|D_c|+N_i}
\end{eqnarray}


