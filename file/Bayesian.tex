\section{贝叶斯分类器}
\subsection{贝叶斯决策论}
假设有$N$种可能的类别标记，即$\mathcal{Y}=\{c_1,c_2,\cdots,c_N\}$，$\lambda_{ij}$为$c_j$的样本误分类为$c_i$所产生的损失。基于后验概率$p(c_i|x)$可获得将样本$x$分类为$c_i$产生的期望损失（expected loss），即在样本$x$上的“条件风险”（conditional risk）
\begin{eqnarray}
R(c_i|x)=\sum_{j=1}^N \lambda_{ij}p(c_j|x)
\end{eqnarray}
条件风险可以理解为，对于给定的$x$，其真实是$c_j$类的概率为$p(c_j|x)$。则对于这个$x$来，猜测是属于$c_i$类，则$\lambda_{ij}$为$c_j$的样本误分类为$c_i$所产生的损失。

任务是寻找一个判定准则$h$，来最小化总体风险
\begin{eqnarray}
R(h)=E[R(h(x)|x)]
\end{eqnarray}
其中，$E[]$为数学期望。贝叶斯判定准则（Bayes decision rule）为：为最小化总体风险，只需在每个样本上选择能使条件风险$R(c|x)$最小的类别标记，即
\begin{eqnarray}
h^\star(x)=\arg\min_{c\in\mathcal{Y}}R(c|x)
\end{eqnarray}
此时$h^\star$成为贝叶斯最优分类器（Bayes optimal classifer），对应的总体风险为$R(h^\star)$称为贝叶斯风险（Bayes risk）。$1-R(h^\star)$反映了分类器所能达到的最好性能。 

具体的，令
\begin{eqnarray}
\lambda_{ij}=
\left\lbrace
\begin{aligned}
0 &,\ \ if\ i=j\\
1 &,\ \ otherwise
\end{aligned}
\right.
\end{eqnarray}
则对于$x$，条件风险为
\begin{eqnarray}
R(c_i|x) &=& \sum_{j=1}^N\lambda_{ij}p(c_j|x)\\
&=& \sum_{j=1}^N 1(c_i\neq c_j)p(c_j|x)\\
&=& 1-p(c_i|x)
\end{eqnarray}
其中，$c_i\in \mathcal{Y}$，可简写为
\begin{eqnarray}
R(c|x)=1-p(c|x)
\end{eqnarray}
因而贝叶斯最优分类器的求解为
\begin{eqnarray}
h^\star(x)=\arg\max_{c\in\mathcal{Y}}p(c|x)
\end{eqnarray}
需要估计$p(c|x)$，有以下两种策略
\begin{itemize}
\item 对于给定$x$，通过直接建模$p(c|x)$来预测$c$，这样得到的是判别式模型（discrimininative models）
\item 对联合概率分布$p(x,c)$建模，然后再由此得到$p(c|x)$，这样得到的是生成式模型（generative models）
\end{itemize}
对于生成式模型，考虑
\begin{eqnarray}
p(c|x) &=& \frac{p(x,c)}{p(c)}\\
&=& \frac{p(c)p(x|c)}{p(x)}
\end{eqnarray}
我们称$p(c)$为类的先验（prior）概率；$p(x|c)$样本$x$相对于类$c$的类条件概率（class-conditional probability）或称为似然（likelihood）；$p(x)$为用于归一化的证据（evidence）因子。由于$p(x)$与类标记无关，因而问题转化为求$p(c)$和$p(x|c)$。

对于类先验概率$p(c)$的估计，可用各类样本出现的频率来进行估计。

对于类条件概率$p(x|c)$的估计，涉及到关于$x$的所有属性的联合概率分布，样本的属性组合数通常大于训练样本数，且很多情况是不会在训练集中出现，因而直接根据样本出现的频率来估计有严重的困难，因而有下列求法。

\subsection{极大似然估计}
估计$p(x|c)$，设$D_c$表示训练集$D$中第$c$类样本组合成的集合，设$D_c$中的样本服从某个分布（如高斯分布），被参数向量$\theta_c$唯一确定，记$p(x|\theta_c)$。则通过极大似然估计求解出$\theta_c$，则可得到$D_c$的样本所服从的分布，则解出$p(x|c)$。

估计$p(x|c)$，设$D_c$表示训练集$D$中第$c$类样本组合成的集合，且假设这些样本是独立同分布的，则参数$\theta_c$对于数据集$D_c$的似然是
\begin{eqnarray}
L(\theta_c)=p(D_c|\theta_c)=\prod_{x\in D_c}p(x|\theta_c)
\end{eqnarray}
取对数似然（log-likelihood），有
\begin{eqnarray}
\begin{aligned}
LL(\theta_c) &= \log p(D_c|\theta_c)\\
&=\sum_{x\in D_c}\log p(x|\theta_c)
\end{aligned}
\end{eqnarray}
参数$\theta_c$的极大似然估计$\hat{\theta_c}$为
\begin{eqnarray}
\hat{\theta_c}=\arg\max_{\theta_c}LL(\theta_c)
\end{eqnarray}

假设$p(x|c)$服从多元高斯分布，则$p(x|c)\sim \mathcal{N}(\mu_c,\Sigma_c)$，参数$\mu_c$和$\Sigma_c$的极大似然估计为
\begin{eqnarray}
\hat{\mu_c}&=&\frac{1}{|D_c|}\sum_{x\in D_c}x\\
\hat{\Sigma_c}&=&\frac{1}{|D_c|}\sum_{x\in D_c}(x-\hat{\mu_c})^T(x-\hat{\mu_c})
\end{eqnarray}
则有
\begin{eqnarray}
p(x|c)=\frac{1}{(2\pi)^{\frac{m}{2}}}\frac{1}{|\hat{\Sigma_c}|^\frac{1}{2}}\exp
\left\lbrace
-\frac{1}{2}(x-\hat{\mu_c})^T\hat{\Sigma_c}^{-1}(x-\hat{\mu_c})\right\rbrace
\end{eqnarray}
其中，$m$指$x$的维度。

这种参数化的方法可以使类条件概率估计变得相对简单，但估计结果的准确性依赖于所猜测的分布是不是对的，这旺旺需要一定程度上利用关于应用任务本身的经验知识。否则若仅凭猜测来假设概率分布，往往得到误导性的结果。

\subsection{朴素贝叶斯分类器}
朴素贝叶斯分类器采用了“属性条件独立性假设（attribute conditional independence assumption）”：对已知类别，假设所有属性相对独立，即假设每个属性独立地对分类结果产生影响。则有
\begin{eqnarray}
\begin{aligned}
p(c|x)&=\frac{p(c)p(x|c)}{p(x)}\\
&=\frac{p(c)}{p(x)}\prod_{i=1}^dp(x_i|c)
\end{aligned}
\end{eqnarray}
其中，$d$为属性数目，$x_i$为$x$在第$i$个属性上的取值。由于$p(x)$不依赖于$c$，则最优的贝叶斯分类器为
\begin{eqnarray}
h(x)=\arg\max_{c\in\mathfrak{Y}}p(c)\prod_{i=1}^dp(x_i|c)
\end{eqnarray}
训练过程即为通过训练集$D$来估计类先验概率$p(c)$，并对每个属性估计条件概率$p(x_i|c)$

对于$p(c)$的求解，令$D_c$表示训练集$D$中第$c$类样本组成的集合，若有充足的独立同分布样本，则$p(c)$可估计为
\begin{eqnarray}
p(c)=\frac{|D_c|}{|D|}
\end{eqnarray}

对于$p(x_i|c)$的求解，分为两种情况，分别为离散属性和连续属性
\begin{itemize}
\item 对于离散属性而言，令$D_{c,x_i}$表示$D_c$中第$i$个属性上取值为$x_i$的样本组成的集合，则条件概率$p(x_i|c)$可估计为
\begin{eqnarray}
p(x_i|c)=\frac{|D_{c,x_i}|}{|D_c|}
\end{eqnarray}
\item 对连续属性而言，可考虑概率密度函数，假定$p(x_i,c)\sim\mathcal{N}(\mu_{c,i},\sigma_{c,i}^2)$。其中，$\mu_{c,i},\sigma_{c,i}^2$分别为第$c$类样本在第$i$个属性上取值的均值和方差，则有
\begin{eqnarray}
p(x_i,c)=\frac{1}{\sqrt{2\pi}\sigma_{c,i}}\exp
\left\lbrace
\begin{aligned}
-\frac{(x_i-\mu_{c,i})^2}{2\sigma_{c,i}^2}
\end{aligned}
\right\rbrace
\end{eqnarray}
\end{itemize}
为了避免其他属性携带的信息因为在训练集中未出现的属性值“抹去”，在估计概率值时通常要进行“平滑（smoothing）”，常用“拉普拉斯修正（Laplacian correction）”。令$N$表示训练集$D$中可能的类别数，$N_i$表示第$i$个属性可能的取值数，则将上式修正为
\begin{eqnarray}
\hat{p(c)}&=&\frac{|D_c|+1}{|D|+N}\\
\hat{p(x_i|c)}&=&\frac{|D_{c,x_i}|+1}{|D_c|+N_i}
\end{eqnarray}

\subsection{半朴素贝叶斯分类器}
半朴素贝叶斯的思想是适当考虑一部分属性间的相互依赖信息，从而既不用计算联合概率分布，又不会彻底忽略较强的属性依赖关系。“独依赖估计（One-Dependent Estimator（ODE））”是一种常用策略，假设每个属性在类别之外最多依赖于一个其他属性，即
\begin{eqnarray}
\begin{aligned}
p(c|x)&=\frac{p(c)p(x|c)}{p(x)}\\
&=\frac{1}{p(x)}p(c)p(x_1x_2\cdots x_d|c)\\
&=\frac{1}{p(x)}p(c)\prod_{i=1}^dp(x_i|c,pa_i)p(pa_i|c)\\
\end{aligned}
\end{eqnarray}
其中，$pa_i$为属性$x_i$所依赖的属性，称为$pa_i$的父属性。若$pa_i$已知，则可用以下公式计算
\begin{eqnarray}
p(c)&=&\frac{|D_c|+1}{|D|+N}\\
p(x_i|c,pa_i)&=&\frac{|D_{c,x_i,pa_i}|+1}{|D_{c,pa_i}|+N_i}
\end{eqnarray}
因而，问题转化为如何确定每个属性的父属性。

\subsubsection{SPODE}
一种最直接的做法是假设所有属性都依赖于同一个属性，这个属性称为“超父（super-parent）”，然后通过交叉验证等模型选择方法来确定超父属性。将此方法称为
SPODE（Super-Parent ODE）。设$pa_i$是$x_1$，则
\begin{eqnarray}
	\begin{aligned}
        	p(c|x)&=\frac{p(c)}{p(x)}\prod_{i=1}^dp(x_i|c,x_1)p(x_1|c)\\
                &=\frac{p(c)p(x_1|c)}{p(x)}\prod_{i=1}^dp(x_i|c,x_1)\\
		&=\frac{p(c,x_1)}{p(x)}\prod_{i=1}^dp(x_i|c,x_1)\\
	\end{aligned}	
\end{eqnarray}
\begin{eqnarray}
        p(x_i|c,x_1)&=&\frac{|D_{c,x_i,x_1}|+1}{|D_{c,x_1}|+N_i}\\
	p(c,x_1)&=&\frac{|D_{c,x_1}|+1}{|D|+N_1}	
\end{eqnarray}

\subsubsection{TAN}
TAN（Tree Augmented na\"ive Bayes)采用最大带权生成树算法，将属性间的依赖关系约简为树形结构
\begin{itemize}
\item[1] 计算任意两个属性间的条件互信息
\begin{eqnarray}
I(x_i,x_j|y)=\sum_{c\in \mathcal{Y}}p(x_i,x_j|c)\log\frac{p(x_i,x_j|c)}{p(x_i|c)p(x_j|c)}
\end{eqnarray}
\item[2] 以属性为节点构建完全图，任意两个节点之间边的权重设为$I(x_i,x_j|y)$
\item[3] 构建完全图的最大带权生成树，挑选跟变量，将边置为有向。对于构建最大带权生成树的算法，可考虑prim（普里姆）或Kruskra（克里斯卡尔）对于最小生成树上的应用，将每次选取最小边换为每次选取最大边即可。
\item[4] 加入类别节点$y$，增加从$y$到每个属性的有向边。
\end{itemize}
容易看出，条件互信息$I(x_i,x_j|y)$刻画了属性$x_i$和$x_j$在已知类别情况下的相关性，通过最大生成树算法，TAN值保留了强相关属性间的依赖性。

\subsubsection{AODE}
AODE（Averaged One-Dependent Estimator）是基于集成学习机制的更强大的独依赖分类器。AODE尝试将每个属性作为超父来构建SPODE，将具有超过阈值数的训练数据支撑的SPODE集成起来作为最终结果，即
\begin{eqnarray}
	p(c|x)=\sum_{j=1,|D_{x_j}|\geq m'}^d \frac{p(c,x_j)}{p(x)}\prod_{i=1}^dp(x_i|c,x_j)\\
\end{eqnarray}
其中，$D_{x_i}$为第$i$个属性上取值为$x_i$的样本的集合，$m'$为集合数量阈值常数，$p(c,x_j)$和$p(x_i|c,x_j)$估计为
\begin{eqnarray}
	\hat{p}(c,x_j)&=&\frac{|D_{c,x_j}|+1}{|D|+N_i}\\
	\hat{p}(x_i|c,x_j)&=&\frac{|D_{c,x_j,x_i}|+1}{|D_{c,x_j}|+N_i}
\end{eqnarray}
其中，$N_i$是第$i$个属性可能的取值数，$D_{c,x_j}$是类别为$c$且第$i$个属性上取值为$x_i$的样本集合，$D_{c,x_j,x_i}$为类别为$c$且第$j$和第$i$个属性上取值分别为$x_j$和$x_i$的样本集合。


















